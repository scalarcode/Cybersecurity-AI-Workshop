{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f165c60f",
      "metadata": {
        "id": "f165c60f"
      },
      "source": [
        "# Final Colab Notebook: Multi-Agent Vulnerability Detection & Response\n",
        "\n",
        "**Overview:** This notebook demonstrates a lightweight multi-agent CAI pipeline using Bandit + Semgrep (Scanner), a Hugging Face model for Analyst & Responder, and a Streamlit UI exposed via ngrok.\n",
        "\n",
        "**Important:** Use synthetic code only. Do NOT paste production secrets. You'll be prompted securely for your Hugging Face token and ngrok authtoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b38e2f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b38e2f7",
        "outputId": "e5a28e4a-0d08-4b2d-ea61-7c5f84de35c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.8/133.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.7/100.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.25.1 requires rich>=13.7.1, but you have rich 13.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalled dependencies\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet streamlit pyngrok huggingface_hub bandit semgrep reportlab\n",
        "print('Installed dependencies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5171c96f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5171c96f",
        "outputId": "f36343ec-3186-4979-c1b5-0ce53bd2fbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face API token (hf_...): ··········\n",
            "Hugging Face client ready. Model set to HuggingFaceH4/zephyr-7b-beta\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import getpass, os\n",
        "\n",
        "HF_TOKEN = getpass.getpass('Enter your Hugging Face API token (hf_...): ' )\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN  # stored for main.py to read\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "MODEL = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "print('Hugging Face client ready. Model set to', MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b468e50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b468e50",
        "outputId": "b0171541-7bf3-40a2-ae5d-084933b3ed1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created sample_app/vulnerable.py\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 566 Oct  4 18:24 vulnerable.py\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "mkdir -p sample_app\n",
        "cat > sample_app/vulnerable.py <<'PY'\n",
        "import os\n",
        "import sqlite3\n",
        "\n",
        "# Hardcoded secret (Bandit should flag this)\n",
        "API_KEY = '12345-SECRET-KEY'\n",
        "\n",
        "def run_command(user_input):\n",
        "    # Command injection vulnerability\n",
        "    os.system('echo ' + user_input)\n",
        "\n",
        "def insecure_db_lookup(user):\n",
        "    # SQL injection vulnerability\n",
        "    conn = sqlite3.connect('users.db')\n",
        "    cur = conn.cursor()\n",
        "    query = \"SELECT * FROM users WHERE username = '%s'\" % user\n",
        "    cur.execute(query)\n",
        "    return cur.fetchall()\n",
        "\n",
        "def main():\n",
        "    u = input('name: ' )\n",
        "    run_command(u)\n",
        "    insecure_db_lookup(u)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "PY\n",
        "echo 'Created sample_app/vulnerable.py' && ls -l sample_app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write main.py (deterministic pipeline + caching + PDF generator)\n",
        "%%writefile main.py\n",
        "import os, json, subprocess, hashlib, re, datetime\n",
        "from huggingface_hub import InferenceClient\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "# ======================================================\n",
        "# Setup\n",
        "# ======================================================\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError(\"HF_TOKEN not set. Please run: os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
        "\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "CACHE_DIR = \"analysis_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# Helpers\n",
        "# ======================================================\n",
        "def run_cmd(cmd):\n",
        "    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    return p.stdout\n",
        "\n",
        "def canonicalize(obj):\n",
        "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
        "\n",
        "def hash_findings(bandit_json, semgrep_json):\n",
        "    return hashlib.sha256((canonicalize(bandit_json) + canonicalize(semgrep_json)).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def load_cache(key):\n",
        "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "def save_cache(key, data):\n",
        "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ======================================================\n",
        "# Scanners\n",
        "# ======================================================\n",
        "def run_bandit():\n",
        "    out = run_cmd(\"bandit -r sample_app -f json -o - 2>/dev/null\")\n",
        "    try:\n",
        "        return json.loads(out)\n",
        "    except Exception:\n",
        "        return {\"results\": []}\n",
        "\n",
        "def run_semgrep():\n",
        "    out = run_cmd(\"semgrep --config=p/ci sample_app --json 2>/dev/null\")\n",
        "    try:\n",
        "        return json.loads(out)\n",
        "    except Exception:\n",
        "        out2 = run_cmd(\"semgrep --config auto sample_app --json 2>/dev/null\")\n",
        "        try:\n",
        "            return json.loads(out2)\n",
        "        except Exception:\n",
        "            return {\"results\": []}\n",
        "\n",
        "# ======================================================\n",
        "# Prompts\n",
        "# ======================================================\n",
        "SYSTEM_PROMPT = \"You are a cybersecurity assistant. Be concise, structured, and factual.\"\n",
        "\n",
        "ANALYST_PROMPT_TEMPLATE = \"\"\"You are an AI Security Analyst.\n",
        "Analyze the vulnerability reports below from Bandit and Semgrep.\n",
        "\n",
        "Prioritize vulnerabilities (High/Medium/Low) and explain their impact briefly.\n",
        "Provide structured, concise output — no exploit details.\n",
        "\n",
        "Bandit findings:\n",
        "{bandit}\n",
        "Semgrep findings:\n",
        "{semgrep}\n",
        "\n",
        "Return JSON with:\n",
        "findings: list of objects with\n",
        "  id, file, line, tool, issue_text, severity, rationale, remediation.\n",
        "\"\"\"\n",
        "\n",
        "# === 🧠 AI Recommendation Agent prompt ===\n",
        "RECOMMENDATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an AI Security Recommendation Agent.\n",
        "\n",
        "Based on the analysis below, provide a detailed, developer-focused plan that includes:\n",
        "1. Concrete fixes or mitigations for each vulnerability.\n",
        "2. Secure code examples or library/config recommendations.\n",
        "3. Prioritized order of fixes if multiple issues exist.\n",
        "4. A short developer message for use in a PR/ticket.\n",
        "5. Optional: Structured ticket metadata (title, severity, labels).\n",
        "\n",
        "Analysis JSON:\n",
        "{analysis}\n",
        "\n",
        "Return JSON with:\n",
        "plan: list of objects with id, recommendation, example(optional), priority\n",
        "ticket: object with title, message, severity, labels\n",
        "\"\"\"\n",
        "\n",
        "# ======================================================\n",
        "# LLM Calls\n",
        "# ======================================================\n",
        "def call_analyst_and_recommender(bandit_json, semgrep_json):\n",
        "    key = hash_findings(bandit_json, semgrep_json)\n",
        "    cached = load_cache(key)\n",
        "    if cached:\n",
        "        return cached\n",
        "\n",
        "    bandit_str = json.dumps(bandit_json.get(\"results\", []), sort_keys=True, ensure_ascii=False)\n",
        "    semgrep_str = json.dumps(semgrep_json.get(\"results\", []), sort_keys=True, ensure_ascii=False)\n",
        "    analyst_prompt = ANALYST_PROMPT_TEMPLATE.format(bandit=bandit_str, semgrep=semgrep_str)\n",
        "\n",
        "    # === Analyst Agent ===\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                      {\"role\": \"user\", \"content\": analyst_prompt}],\n",
        "            max_tokens=900, temperature=0.0, top_p=1.0\n",
        "        )\n",
        "        text = resp.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        text = \"\"\n",
        "\n",
        "    try:\n",
        "        analysis_json = json.loads(re.search(r'(\\{.*\\})', text, re.DOTALL).group(1))\n",
        "    except Exception:\n",
        "        analysis_json = {\"findings\": []}\n",
        "        for i, r in enumerate(bandit_json.get(\"results\", []), start=1):\n",
        "            analysis_json[\"findings\"].append({\n",
        "                \"id\": f\"bandit-{i}\",\n",
        "                \"file\": r.get(\"filename\"),\n",
        "                \"line\": (r.get(\"line_range\") or None),\n",
        "                \"tool\": \"bandit\",\n",
        "                \"issue_text\": r.get(\"test_name\"),\n",
        "                \"severity\": \"Medium\",\n",
        "                \"rationale\": r.get(\"issue_text\", \"\")[:200],\n",
        "                \"remediation\": \"Review code and follow secure coding guidance.\"\n",
        "            })\n",
        "\n",
        "    # === Recommendation Agent ===\n",
        "    analysis_text = json.dumps(analysis_json, ensure_ascii=False)\n",
        "    rec_prompt = RECOMMENDATION_PROMPT_TEMPLATE.format(analysis=analysis_text)\n",
        "\n",
        "    try:\n",
        "        resp2 = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                      {\"role\": \"user\", \"content\": rec_prompt}],\n",
        "            max_tokens=1000, temperature=0.0, top_p=1.0\n",
        "        )\n",
        "        text2 = resp2.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        text2 = \"\"\n",
        "\n",
        "    try:\n",
        "        recommendation_json = json.loads(re.search(r'(\\{.*\\})', text2, re.DOTALL).group(1))\n",
        "    except Exception:\n",
        "        recommendation_json = {\"plan\": [], \"ticket\": {\"title\": \"Remediation Plan\", \"message\": \"Fix security findings\", \"severity\": \"Medium\", \"labels\": [\"security\"]}}\n",
        "        for f in analysis_json.get(\"findings\", []):\n",
        "            recommendation_json[\"plan\"].append({\n",
        "                \"id\": f.get(\"id\"),\n",
        "                \"recommendation\": f\"Fix {f.get('issue_text')} in {f.get('file')}\",\n",
        "                \"priority\": f.get(\"severity\", \"Medium\")\n",
        "            })\n",
        "\n",
        "    out = {\"analysis_json\": analysis_json, \"recommendation_json\": recommendation_json}\n",
        "    save_cache(key, out)\n",
        "    return out\n",
        "\n",
        "# ======================================================\n",
        "# PDF Report\n",
        "# ======================================================\n",
        "def generate_pdf_report(bandit_text, semgrep_text, analysis_text, recommendation_text, output_path=\"CyberAI_Report.pdf\"):\n",
        "    styles = getSampleStyleSheet()\n",
        "    doc = SimpleDocTemplate(output_path, pagesize=A4)\n",
        "    story = []\n",
        "\n",
        "    story.append(Paragraph(\"<b>Cybersecurity AI Report</b>\", styles[\"Title\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph(f\"Generated: {datetime.datetime.utcnow().isoformat()} UTC\", styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>Bandit Findings</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(f\"<pre>{bandit_text[:4000]}</pre>\", styles[\"Code\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>Semgrep Findings</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(f\"<pre>{semgrep_text[:4000]}</pre>\", styles[\"Code\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>AI Analysis</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(analysis_text.replace('\\n','<br/>'), styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>AI Recommendations</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(recommendation_text.replace('\\n','<br/>'), styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    doc.build(story)\n",
        "    return output_path\n",
        "\n",
        "# ======================================================\n",
        "# Coordinator\n",
        "# ======================================================\n",
        "def coordinator():\n",
        "    bandit_json = run_bandit()\n",
        "    semgrep_json = run_semgrep()\n",
        "\n",
        "    bandit_sorted = {\"results\": sorted(bandit_json.get(\"results\", []), key=lambda r: (r.get(\"filename\",\"\"), r.get(\"test_name\",\"\")))}\n",
        "    semgrep_sorted = {\"results\": sorted(semgrep_json.get(\"results\", []), key=lambda r: (r.get(\"path\",\"\"), r.get(\"check_id\",\"\")))}\n",
        "\n",
        "    out = call_analyst_and_recommender(bandit_sorted, semgrep_sorted)\n",
        "    analysis_json = out.get(\"analysis_json\", {})\n",
        "    recommendation_json = out.get(\"recommendation_json\") or out.get(\"responder_json\", {})\n",
        "\n",
        "    # ---- Text formatting ----\n",
        "    analysis_lines = []\n",
        "    for f in analysis_json.get(\"findings\", []):\n",
        "        analysis_lines.append(\n",
        "            f\"[{f.get('severity')}] {f.get('file')}:{f.get('line')} - {f.get('issue_text')} | Remediation: {f.get('remediation')}\"\n",
        "        )\n",
        "    analysis_text = \"\\n\".join(analysis_lines) or json.dumps(analysis_json, indent=2)\n",
        "\n",
        "    rec_lines = []\n",
        "    if isinstance(recommendation_json, dict):\n",
        "        if \"plan\" in recommendation_json:\n",
        "            for step in recommendation_json[\"plan\"]:\n",
        "                rec_lines.append(f\"- {step.get('id')}: {step.get('recommendation')} (Priority: {step.get('priority')})\")\n",
        "        if \"ticket\" in recommendation_json:\n",
        "            rec_lines.append(f\"\\nTicket: {json.dumps(recommendation_json['ticket'], indent=2)}\")\n",
        "    recommendation_text = \"\\n\".join(rec_lines) or json.dumps(recommendation_json, indent=2)\n",
        "\n",
        "    return bandit_sorted, semgrep_sorted, analysis_text, recommendation_text, analysis_json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp9U8pimhPId",
        "outputId": "fc0098aa-00f2-40f4-983b-4ad2101886e0"
      },
      "id": "Dp9U8pimhPId",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write app.py (Streamlit UI)\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import json\n",
        "from main import coordinator, generate_pdf_report\n",
        "\n",
        "st.set_page_config(page_title=\"CAI Vulnerability Demo (Deterministic)\", layout=\"wide\")\n",
        "st.title(\"### Multi-Agent CAI Framework Demo (Scanner + Analyst + Responder)\")\n",
        "\n",
        "st.markdown(\"Click **Run Multi-Agent Scan** to run Bandit+Semgrep, have the AI analyze, and generate a PDF.\")\n",
        "\n",
        "if st.button(\"🚀 Run Multi-Agent Scan\"):\n",
        "    with st.spinner(\"Running scanners and AI agents...\"):\n",
        "        bjson, sjson, analysis_text, responder_text, analysis_json = coordinator()\n",
        "\n",
        "    st.subheader(\"🔍 Bandit (raw JSON)\")\n",
        "    st.json(bjson)\n",
        "\n",
        "    st.subheader(\"🔍 Semgrep (raw JSON)\")\n",
        "    st.json(sjson)\n",
        "\n",
        "    st.subheader(\"🧠 AI Analyst\")\n",
        "    st.code(analysis_text, language=\"text\")\n",
        "\n",
        "    st.subheader(\"🛠 AI Responder\")\n",
        "    st.code(responder_text, language=\"text\")\n",
        "\n",
        "    # Generate PDF and offer download\n",
        "    report_path = generate_pdf_report(json.dumps(bjson, indent=2, ensure_ascii=False),\n",
        "                                      json.dumps(sjson, indent=2, ensure_ascii=False),\n",
        "                                      analysis_text,\n",
        "                                      responder_text,\n",
        "                                      output_path=\"CyberAI_Report.pdf\")\n",
        "    with open(report_path, \"rb\") as f:\n",
        "        st.download_button(\"📄 Download report (PDF)\", f, file_name=\"CyberAI_Report.pdf\", mime=\"application/pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEghy2FLZWTQ",
        "outputId": "bbe2ff76-2dbb-45a1-d68a-8a46424599b0"
      },
      "id": "NEghy2FLZWTQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch Streamlit and ngrok (pyngrok). Enter ngrok token when prompted.\n",
        "!pip install --quiet pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass, subprocess, time, os\n",
        "\n",
        "NGROK_TOKEN = getpass.getpass(\"Enter ngrok authtoken (optional, press Enter to skip): \")\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# start streamlit in background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"10000\"])\n",
        "time.sleep(2)\n",
        "url = ngrok.connect(10000)\n",
        "print(\"Open the Streamlit app at:\", url.public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mww1D8rZ0g0",
        "outputId": "84090b7f-c693-4f50-9605-60379650767d"
      },
      "id": "1mww1D8rZ0g0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter ngrok authtoken (optional, press Enter to skip): ··········\n",
            "Open the Streamlit app at: https://nonprominently-rendible-britni.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Kill old tunnels\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "57wV8d7oVBve"
      },
      "id": "57wV8d7oVBve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This clears all cached JSONs, ensuring the new Recommendation Agent regenerates deterministic output fresh.\n",
        "!rm -rf analysis_cache"
      ],
      "metadata": {
        "id": "ag_sfnZMiU_p"
      },
      "id": "ag_sfnZMiU_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "998708e2",
      "metadata": {
        "id": "998708e2"
      },
      "source": [
        "### Troubleshooting\n",
        "\n",
        "- If you see `401 Unauthorized` from Hugging Face: re-generate your HF token and re-run the HF token cell.\n",
        "\n",
        "- If ngrok fails: ensure you set an ngrok token or try re-running the ngrok cell.\n",
        "\n",
        "- If `main` import fails: ensure you executed the cell that wrote `main.py` before running app cells.\n",
        "\n",
        "### Quick run checklist\n",
        "\n",
        "1. Run the install cell.\n",
        "2. Run the HF token cell and paste token securely.\n",
        "3. Run the sample_app creation cell.\n",
        "4. Run the `main.py` and `app.py` write cells.\n",
        "5. Run the ngrok launch cell and open the printed public URL.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}