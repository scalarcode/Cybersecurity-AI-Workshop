{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f165c60f",
      "metadata": {
        "id": "f165c60f"
      },
      "source": [
        "# Final Colab Notebook: Multi-Agent Vulnerability Detection & Response\n",
        "\n",
        "**Overview:** This notebook demonstrates a lightweight multi-agent CAI pipeline using Bandit + Semgrep (Scanner), a Hugging Face model for Analyst & Responder, and a Streamlit UI exposed via ngrok.\n",
        "\n",
        "**Important:** Use synthetic code only. Do NOT paste production secrets. You'll be prompted securely for your Hugging Face token and ngrok authtoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b38e2f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b38e2f7",
        "outputId": "e96c3b1c-9660-4096-846b-6ccc61073f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.8/133.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100.7/100.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.25.1 requires rich>=13.7.1, but you have rich 13.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalled dependencies\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet streamlit pyngrok huggingface_hub bandit semgrep reportlab\n",
        "print('Installed dependencies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5171c96f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5171c96f",
        "outputId": "4d89351f-9d65-48bb-c3e4-4ece49bb57fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face API token (hf_...): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Hugging Face client ready. Model set to HuggingFaceH4/zephyr-7b-beta\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import getpass, os\n",
        "\n",
        "HF_TOKEN = getpass.getpass('Enter your Hugging Face API token (hf_...): ' )\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN  # stored for main.py to read\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "MODEL = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "print('Hugging Face client ready. Model set to', MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > custom_rules.yaml <<'YAML'\n",
        "rules:\n",
        "  - id: insecure-os-system\n",
        "    patterns:\n",
        "      - pattern: os.system($CMD)\n",
        "    message: \"Possible command injection via os.system\"\n",
        "    severity: ERROR\n",
        "    languages: [python]\n",
        "    metadata:\n",
        "      cwe: \"CWE-78: Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')\"\n",
        "\n",
        "  - id: subprocess-shell-true\n",
        "    patterns:\n",
        "      - pattern: subprocess.call($ARGS, shell=True)\n",
        "    message: \"Use of subprocess with shell=True can lead to command injection\"\n",
        "    severity: ERROR\n",
        "    languages: [python]\n",
        "    metadata:\n",
        "      cwe: \"CWE-78\"\n",
        "\n",
        "  - id: sql-injection\n",
        "    patterns:\n",
        "      - pattern: conn.execute(\"SELECT\" + $VAR)\n",
        "      - pattern: conn.cursor().execute(\"SELECT\" + $VAR)\n",
        "      - pattern: conn.cursor().execute(\"SELECT * FROM users WHERE id = '%s'\" % $VAR)\n",
        "    message: \"SQL query uses untrusted string concatenation ‚Äî potential SQL injection\"\n",
        "    severity: ERROR\n",
        "    languages: [python]\n",
        "    metadata:\n",
        "      cwe: \"CWE-89: SQL Injection\"\n",
        "\n",
        "  - id: unsafe-pickle-loads\n",
        "    patterns:\n",
        "      - pattern: pickle.loads($DATA)\n",
        "    message: \"Insecure deserialization using pickle.loads\"\n",
        "    severity: WARNING\n",
        "    languages: [python]\n",
        "    metadata:\n",
        "      cwe: \"CWE-502: Deserialization of Untrusted Data\"\n",
        "YAML\n",
        "echo \"‚úÖ Created custom_rules.yaml\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfaNX0Ck_uVo",
        "outputId": "65fbf071-31a4-4651-bdb9-48a8809d14e1"
      },
      "id": "qfaNX0Ck_uVo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created custom_rules.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p sample_app\n",
        "cat > sample_app/vulnerable.py <<'PY'\n",
        "# sample_app/vulnerable.py\n",
        "# Intentionally insecure Python app for Bandit & Semgrep demo.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sqlite3\n",
        "import pickle\n",
        "import requests\n",
        "\n",
        "API_SECRET = \"hardcoded-demo-secret\"  # [Bandit B105] Hardcoded password/secret\n",
        "\n",
        "def get_user_data(user_id):\n",
        "    # SQL Injection vulnerability ‚Äî string concatenation\n",
        "    conn = sqlite3.connect(\"users.db\")\n",
        "    cur = conn.cursor()\n",
        "    query = \"SELECT * FROM users WHERE id = '%s'\" % user_id  # flagged by Semgrep\n",
        "    cur.execute(query)\n",
        "    return cur.fetchall()\n",
        "\n",
        "def execute_user_input(cmd):\n",
        "    # Command injection ‚Äî flagged by Bandit & Semgrep\n",
        "    os.system(\"ping -c 1 \" + cmd)\n",
        "\n",
        "def unsafe_pickle(data):\n",
        "    # Unsafe deserialization ‚Äî flagged by Semgrep\n",
        "    return pickle.loads(data)\n",
        "\n",
        "def unverified_request(url):\n",
        "    # SSL verification disabled ‚Äî flagged by Bandit\n",
        "    return requests.get(url, verify=False)\n",
        "\n",
        "def run_shell(user_input):\n",
        "    # Another command injection example (for Semgrep pattern match)\n",
        "    subprocess.call(f\"echo {user_input}\", shell=True)\n",
        "\n",
        "def main():\n",
        "    user_input = input(\"Enter username: \")\n",
        "    execute_user_input(user_input)\n",
        "    get_user_data(user_input)\n",
        "    unsafe_pickle(b\"cos\\nsystem\\n(S'ls'\\ntR.\")\n",
        "    unverified_request(\"https://example.com\")\n",
        "    run_shell(user_input)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n",
        "echo \"‚úÖ Created sample_app/vulnerable.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F77ZMWyiARtF",
        "outputId": "0fbff487-6bc7-4df1-c1e8-d86e9aa16bfa"
      },
      "id": "F77ZMWyiARtF",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created sample_app/vulnerable.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write main.py (deterministic pipeline + caching + PDF generator)\n",
        "%%writefile main.py\n",
        "import os, json, subprocess, hashlib, re, datetime\n",
        "from huggingface_hub import InferenceClient\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "# ======================================================\n",
        "# Setup\n",
        "# ======================================================\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError(\"HF_TOKEN not set. Please run: os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
        "\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Why \"HuggingFaceH4/zephyr-7b-beta\" model good for Cyber AI ?\n",
        "# - Advanced Threat Detection: Leverages deep learning to identify vulnerabilities with high accuracy.\n",
        "# - Real-Time Analysis: Enables swift automated responses to emerging cyber threats.\n",
        "# - Resource Efficiency: Optimized model size allows deployment on limited hardware.\n",
        "# - Secure Automation: Ensures data privacy and security during vulnerability assessments.\n",
        "# - Scalable Integration: Easily integrates with existing cyber AI frameworks and tools.\n",
        "\n",
        "CACHE_DIR = \"analysis_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ======================================================\n",
        "# Helpers\n",
        "# ======================================================\n",
        "def run_cmd(cmd):\n",
        "    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    return p.stdout\n",
        "\n",
        "def canonicalize(obj):\n",
        "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
        "\n",
        "def hash_findings(bandit_json, semgrep_json):\n",
        "    return hashlib.sha256((canonicalize(bandit_json) + canonicalize(semgrep_json)).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def load_cache(key):\n",
        "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "def save_cache(key, data):\n",
        "    path = os.path.join(CACHE_DIR, f\"{key}.json\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ======================================================\n",
        "# Scanners\n",
        "# ======================================================\n",
        "def run_bandit():\n",
        "    out = run_cmd(\"bandit -r sample_app -f json -o - 2>/dev/null\")\n",
        "    try:\n",
        "        return json.loads(out)\n",
        "    except Exception:\n",
        "        return {\"results\": []}\n",
        "\n",
        "def run_semgrep():\n",
        "    out = run_cmd(\"semgrep --config=p/ci sample_app --json 2>/dev/null\")\n",
        "    try:\n",
        "        js = json.loads(out)\n",
        "        if js.get(\"results\"):\n",
        "            return js\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # fallback\n",
        "    out2 = run_cmd(\"semgrep --config=auto sample_app --json 2>/dev/null\")\n",
        "    try:\n",
        "        return json.loads(out2)\n",
        "    except Exception:\n",
        "        return {\"results\": []}\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Prompts\n",
        "# ======================================================\n",
        "SYSTEM_PROMPT = \"You are a cybersecurity assistant. Be concise, structured, and factual.\"\n",
        "\n",
        "ANALYST_PROMPT_TEMPLATE = \"\"\"You are an AI Security Analyst.\n",
        "Analyze the vulnerability reports below from Bandit and Semgrep.\n",
        "\n",
        "Prioritize vulnerabilities (High/Medium/Low) and explain their impact briefly.\n",
        "Provide structured, concise output ‚Äî no exploit details.\n",
        "\n",
        "Bandit findings:\n",
        "{bandit}\n",
        "Semgrep findings:\n",
        "{semgrep}\n",
        "\n",
        "Return JSON with:\n",
        "findings: list of objects with id, file, line, tool, issue_text, severity, rationale, remediation.\n",
        "\"\"\"\n",
        "\n",
        "# === üß† AI Recommendation Agent prompt (Enhanced) ===\n",
        "RECOMMENDATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an AI Security Recommendation Agent that helps developers fix vulnerabilities.\n",
        "\n",
        "Below is a combined security analysis from multiple tools (Bandit, Semgrep).\n",
        "For each finding, do the following:\n",
        "1. Identify what the issue is and why it is risky.\n",
        "2. Show the vulnerable code snippet or line context (if provided).\n",
        "3. Provide a clear and correct secure fix (with short code example).\n",
        "4. Prioritize findings (High, Medium, Low).\n",
        "5. Provide a developer-friendly explanation and ticket message for PR.\n",
        "\n",
        "Input JSON:\n",
        "{analysis}\n",
        "\n",
        "Return valid JSON with:\n",
        "{{\n",
        "  \"plan\": [\n",
        "    {{\n",
        "      \"id\": \"finding-1\",\n",
        "      \"tool\": \"bandit/semgrep\",\n",
        "      \"file\": \"path/to/file\",\n",
        "      \"line\": number,\n",
        "      \"issue\": \"brief description\",\n",
        "      \"recommendation\": \"how to fix\",\n",
        "      \"example\": \"secure code snippet\",\n",
        "      \"priority\": \"High|Medium|Low\"\n",
        "    }}\n",
        "  ],\n",
        "  \"ticket\": {{\n",
        "    \"title\": \"Security Remediation Plan\",\n",
        "    \"message\": \"Summary for developer\",\n",
        "    \"severity\": \"High|Medium|Low\",\n",
        "    \"labels\": [\"security\", \"code-fix\"]\n",
        "  }}\n",
        "}}\n",
        "Rules:\n",
        "- Always include both Bandit and Semgrep findings if available.\n",
        "- Be specific: show correct secure replacement or code fix.\n",
        "- Focus on Python best practices (avoid shell=True, input validation, parameterized SQL, etc.).\n",
        "\"\"\"\n",
        "\n",
        "def call_analyst_and_recommender(bandit_json, semgrep_json):\n",
        "    key = hash_findings(bandit_json, semgrep_json)\n",
        "    cached = load_cache(key)\n",
        "    if cached:\n",
        "        return cached\n",
        "\n",
        "    bandit_str = json.dumps(bandit_json.get(\"results\", []), sort_keys=True, ensure_ascii=False)\n",
        "    semgrep_str = json.dumps(semgrep_json.get(\"results\", []), sort_keys=True, ensure_ascii=False)\n",
        "    analyst_prompt = ANALYST_PROMPT_TEMPLATE.format(bandit=bandit_str, semgrep=semgrep_str)\n",
        "\n",
        "    # === Analyst Agent ===\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": analyst_prompt}\n",
        "            ],\n",
        "            max_tokens=900,\n",
        "            temperature=0.0,\n",
        "            top_p=1.0\n",
        "        )\n",
        "        text = resp.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        text = \"\"\n",
        "\n",
        "    # --- Try parsing AI response ---\n",
        "    try:\n",
        "        analysis_json = json.loads(re.search(r'(\\{.*\\})', text, re.DOTALL).group(1))\n",
        "    except Exception:\n",
        "        # Fallback: merge Bandit + Semgrep manually\n",
        "        analysis_json = {\"findings\": []}\n",
        "\n",
        "        # Add Bandit results\n",
        "        for i, r in enumerate(bandit_json.get(\"results\", []), start=1):\n",
        "            analysis_json[\"findings\"].append({\n",
        "                \"id\": f\"bandit-{i}\",\n",
        "                \"file\": r.get(\"filename\"),\n",
        "                \"line\": r.get(\"line_number\"),\n",
        "                \"tool\": \"bandit\",\n",
        "                \"issue_text\": r.get(\"issue_text\"),\n",
        "                \"severity\": r.get(\"issue_severity\", \"Medium\"),\n",
        "                \"rationale\": r.get(\"more_info\", \"\"),\n",
        "                \"remediation\": \"Review Bandit documentation and apply secure coding fix.\"\n",
        "            })\n",
        "\n",
        "        # Add Semgrep results too\n",
        "        for j, s in enumerate(semgrep_json.get(\"results\", []), start=1):\n",
        "            meta = s.get(\"extra\", {}).get(\"metadata\", {})\n",
        "            analysis_json[\"findings\"].append({\n",
        "                \"id\": f\"semgrep-{j}\",\n",
        "                \"file\": s.get(\"path\"),\n",
        "                \"line\": s.get(\"start\", {}).get(\"line\"),\n",
        "                \"tool\": \"semgrep\",\n",
        "                \"issue_text\": s.get(\"extra\", {}).get(\"message\"),\n",
        "                \"severity\": s.get(\"extra\", {}).get(\"severity\", \"Medium\"),\n",
        "                \"rationale\": meta.get(\"category\", \"security\"),\n",
        "                \"remediation\": \"Follow Semgrep rule guidance; apply fix or mitigation.\"\n",
        "            })\n",
        "\n",
        "    # === Recommendation Agent ===\n",
        "    analysis_text = json.dumps(analysis_json, ensure_ascii=False)\n",
        "    rec_prompt = RECOMMENDATION_PROMPT_TEMPLATE.format(analysis=analysis_text)\n",
        "\n",
        "    try:\n",
        "        resp2 = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": rec_prompt}\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.0,\n",
        "            top_p=1.0\n",
        "        )\n",
        "        text2 = resp2.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        text2 = \"\"\n",
        "\n",
        "    # --- Try parsing recommendation JSON ---\n",
        "    try:\n",
        "        recommendation_json = json.loads(re.search(r'(\\{.*\\})', text2, re.DOTALL).group(1))\n",
        "    except Exception:\n",
        "        recommendation_json = {\"plan\": [], \"ticket\": {\n",
        "            \"title\": \"Remediation Plan\",\n",
        "            \"message\": \"Fix security findings\",\n",
        "            \"severity\": \"Medium\",\n",
        "            \"labels\": [\"security\"]\n",
        "        }}\n",
        "        for f in analysis_json.get(\"findings\", []):\n",
        "            recommendation_json[\"plan\"].append({\n",
        "                \"id\": f.get(\"id\"),\n",
        "                \"recommendation\": f\"Fix {f.get('issue_text')} in {f.get('file')}\",\n",
        "                \"priority\": f.get(\"severity\", \"Medium\")\n",
        "            })\n",
        "\n",
        "    out = {\n",
        "        \"analysis_json\": analysis_json,\n",
        "        \"recommendation_json\": recommendation_json\n",
        "    }\n",
        "    save_cache(key, out)\n",
        "    return out\n",
        "\n",
        "# ======================================================\n",
        "# PDF Report\n",
        "# ======================================================\n",
        "def generate_pdf_report(bandit_text, semgrep_text, analysis_text, recommendation_text, output_path=\"CyberAI_Report.pdf\"):\n",
        "    styles = getSampleStyleSheet()\n",
        "    doc = SimpleDocTemplate(output_path, pagesize=A4)\n",
        "    story = []\n",
        "\n",
        "    story.append(Paragraph(\"<b>Cybersecurity AI Report</b>\", styles[\"Title\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph(f\"Generated: {datetime.datetime.utcnow().isoformat()} UTC\", styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>Bandit Findings</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(f\"<pre>{bandit_text[:4000]}</pre>\", styles[\"Code\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>Semgrep Findings</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(f\"<pre>{semgrep_text[:4000]}</pre>\", styles[\"Code\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>AI Analysis</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(analysis_text.replace('\\n','<br/>'), styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    story.append(Paragraph(\"<b>AI Recommendations</b>\", styles[\"Heading3\"]))\n",
        "    story.append(Paragraph(recommendation_text.replace('\\n','<br/>'), styles[\"Normal\"]))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    doc.build(story)\n",
        "    return output_path\n",
        "\n",
        "# ======================================================\n",
        "# Coordinator\n",
        "# ======================================================\n",
        "def coordinator():\n",
        "    bandit_json = run_bandit()\n",
        "    semgrep_json = run_semgrep()\n",
        "\n",
        "    bandit_sorted = {\"results\": sorted(bandit_json.get(\"results\", []), key=lambda r: (r.get(\"filename\",\"\"), r.get(\"test_name\",\"\")))}\n",
        "    semgrep_sorted = {\"results\": sorted(semgrep_json.get(\"results\", []), key=lambda r: (r.get(\"path\",\"\"), r.get(\"check_id\",\"\")))}\n",
        "\n",
        "    out = call_analyst_and_recommender(bandit_sorted, semgrep_sorted)\n",
        "    analysis_json = out.get(\"analysis_json\", {})\n",
        "    recommendation_json = out.get(\"recommendation_json\") or out.get(\"responder_json\", {})\n",
        "\n",
        "    # ---- Text formatting ----\n",
        "    analysis_lines = []\n",
        "    for f in analysis_json.get(\"findings\", []):\n",
        "        analysis_lines.append(\n",
        "            f\"[{f.get('severity')}] {f.get('file')}:{f.get('line')} - {f.get('issue_text')} | Remediation: {f.get('remediation')}\"\n",
        "        )\n",
        "    analysis_text = \"\\n\".join(analysis_lines) or json.dumps(analysis_json, indent=2)\n",
        "\n",
        "    rec_lines = []\n",
        "    if isinstance(recommendation_json, dict):\n",
        "        if \"plan\" in recommendation_json:\n",
        "            for step in recommendation_json[\"plan\"]:\n",
        "                rec_lines.append(f\"- {step.get('id')}: {step.get('recommendation')} (Priority: {step.get('priority')})\")\n",
        "        if \"ticket\" in recommendation_json:\n",
        "            rec_lines.append(f\"\\nTicket: {json.dumps(recommendation_json['ticket'], indent=2)}\")\n",
        "    recommendation_text = \"\\n\".join(rec_lines) or json.dumps(recommendation_json, indent=2)\n",
        "\n",
        "    return bandit_sorted, semgrep_sorted, analysis_text, recommendation_text, analysis_json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp9U8pimhPId",
        "outputId": "b3b306fd-1104-46c4-c3fd-27d071b2ca6d"
      },
      "id": "Dp9U8pimhPId",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write app.py (Streamlit UI)\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import json\n",
        "from main import coordinator, generate_pdf_report\n",
        "\n",
        "st.set_page_config(page_title=\"CAI Vulnerability Demo (Deterministic)\", layout=\"wide\")\n",
        "st.title(\"Multi-Agent CAI Framework Demo (Scanner + Analyst + Recommendation)\")\n",
        "\n",
        "st.markdown(\"Click **Run Multi-Agent Scan** to run Bandit+Semgrep, have the AI analyze, and generate a PDF.\")\n",
        "\n",
        "if st.button(\"üöÄ Run Multi-Agent Scan\"):\n",
        "    with st.spinner(\"Running scanners and AI agents...\"):\n",
        "        bjson, sjson, analysis_text, responder_text, analysis_json = coordinator()\n",
        "\n",
        "    st.subheader(\"üîç Bandit (raw JSON)\")\n",
        "    st.json(bjson)\n",
        "\n",
        "    st.subheader(\"üîç Semgrep (raw JSON)\")\n",
        "    st.json(sjson)\n",
        "\n",
        "    st.subheader(\"üß† AI Analyst\")\n",
        "    st.code(analysis_text, language=\"text\")\n",
        "\n",
        "    st.subheader(\"üõ† AI Responder\")\n",
        "    st.code(responder_text, language=\"text\")\n",
        "\n",
        "    # Generate PDF and offer download\n",
        "    report_path = generate_pdf_report(json.dumps(bjson, indent=2, ensure_ascii=False),\n",
        "                                      json.dumps(sjson, indent=2, ensure_ascii=False),\n",
        "                                      analysis_text,\n",
        "                                      responder_text,\n",
        "                                      output_path=\"CyberAI_Report.pdf\")\n",
        "    with open(report_path, \"rb\") as f:\n",
        "        st.download_button(\"üìÑ Download report (PDF)\", f, file_name=\"CyberAI_Report.pdf\", mime=\"application/pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEghy2FLZWTQ",
        "outputId": "22411410-9134-4d67-aea5-97e0ec2e1236"
      },
      "id": "NEghy2FLZWTQ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch Streamlit and ngrok (pyngrok). Enter ngrok token when prompted.\n",
        "!pip install --quiet pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass, subprocess, time, os\n",
        "\n",
        "NGROK_TOKEN = getpass.getpass(\"Enter ngrok authtoken (optional, press Enter to skip): \")\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# start streamlit in background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"10000\"])\n",
        "time.sleep(2)\n",
        "url = ngrok.connect(10000)\n",
        "print(\"Open the Streamlit app at:\", url.public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mww1D8rZ0g0",
        "outputId": "2256fb82-3d7f-4f2e-afd0-2ecabd1582cb"
      },
      "id": "1mww1D8rZ0g0",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter ngrok authtoken (optional, press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Open the Streamlit app at: https://nonprominently-rendible-britni.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Kill old tunnels\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "57wV8d7oVBve"
      },
      "id": "57wV8d7oVBve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This clears all cached JSONs, ensuring the new Recommendation Agent regenerates deterministic output fresh.\n",
        "!rm -rf analysis_cache"
      ],
      "metadata": {
        "id": "ag_sfnZMiU_p"
      },
      "id": "ag_sfnZMiU_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "998708e2",
      "metadata": {
        "id": "998708e2"
      },
      "source": [
        "### Troubleshooting\n",
        "\n",
        "- If you see `401 Unauthorized` from Hugging Face: re-generate your HF token and re-run the HF token cell.\n",
        "\n",
        "- If ngrok fails: ensure you set an ngrok token or try re-running the ngrok cell.\n",
        "\n",
        "- If `main` import fails: ensure you executed the cell that wrote `main.py` before running app cells.\n",
        "\n",
        "### Quick run checklist\n",
        "\n",
        "1. Run the install cell.\n",
        "2. Run the HF token cell and paste token securely.\n",
        "3. Run the sample_app creation cell.\n",
        "4. Run the `main.py` and `app.py` write cells.\n",
        "5. Run the ngrok launch cell and open the printed public URL.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}